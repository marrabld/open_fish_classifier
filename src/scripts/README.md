# Utility Script Usage

All the scripts below use `argparse` to build their CLI; you can view the command line options for each of them with the `-h` flag, i.e. `python get_crop_dataset.py -h`.

### `gen_crop_dataset.py`
This script is used to generate a test/validate/train dataset of fish crop data, grouped by species. The script requires a metadata csv that maps crop images to their species and allows you to specify the desired weighting for the test/validate/train partitions.

Example usage:
```bash
python gen_crop_dataset.py \
  --input ~/fish/crops \
  --output ~/fish/training-crops \
  --weights 80/10/10 \
  --metadata ~/fish/crop_metadata.csv
```

### `gen_frame_dataset.py`
This script is similar to `gen_crop_dataset.py` in that it generates a training dataset, however there are a few key differences. The output dataset structure is specifically structured to work with [ImageAI](https://github.com/OlafenwaMoses/ImageAI)'s training library.

The script utilises a metadata file, provided by AIMS, that details fish labels and bounding boxes for frames in the public frames image set. The script allows you to specify the species you are interested in training on (in `family_genus_species` form), and will merge all other fish under a generic 'fish' label. Each dataset is given a unique name which can be used in other compatible scripts.

Example usage:
```bash
python gen_frame_dataset.py \
  --frame-directory ~/fish/frames \
  --metadata-path ~/fish/frame-metadata.json \
  --species lethrinidae_lethrinus_punctulatus \
  --species lutjanidae_lutjanus_sebae \
  top-two-species
```

*NOTE: the name of the dataset will be used as the name of a directory*

### `yolo_train_frame_dataset.py`
This script uses ImageAI train a YOLOv3 model against a dataset generated by `gen_frame_dataset.py`, utilising transfer learning to allow for training against an existing model (i.e. [pretrained-yolov3.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/pretrained-yolov3.h5)).

Like `gen_frame_dataset.py`, each training run is created with a unique name, which can be used to keep track of previous training runs as well as for feeding in to other scripts (i.e `yolo_evaluate_training.py`).

Example usage:
```bash
python yolo_train_frame_dataset.py \
   --epochs 500 \
   --dataset top-two-species \
   --batch-size 8 \
   --pretrained-path ~/fish/pretrained-yolov3.h5 \
   top-two-species-b8-yolov3

# In a multi-gpu setup you may want to delegate the training to specific GPUs, this can be done by setting the `CUDA_VISIBLE_DEVICES` environment variable. Specific GPU numbers can be found with the `nvidia-smi` command.

CUDA_VISIBLE_DEVICES="0,1" python yolo_train_frame_dataset.py \
   --epochs 500 \
   --dataset top-two-species \
   --batch-size 8 \
   --pretrained-path ~/fish/pretrained-yolov3.h5 \
   top-two-species-b8-yolov3
```

### `yolo_evaluate_training.py`
This script evaluates models produced in a training run created by `yolo_train_frame_dataset.py`. The output results will be sent to stdout, while TensorFlow output is written to stderr, so you will need to redirect these outputs appropriately to view the results. 

The script allows you to run on either the entire model set, or target specific batches of epochs (run `python yolo_evaluate_training.py -h` for more info). Currently it appears that there are issues in the way that ImageAI runs the evaluation over the whole model set and it may OOM your GPU if there are a decent amount of models so you may need to run the evaluation in batches. There is currently work in progress to improve this situation.

Example usage:
```bash
python yolo_evaluate_training.py \
    --start-epoch 30 \
    --total-epochs 10 \
    --iou-threshold 0.5 \
    --nms-threshold 0.5 \
    --object-threshold 0.3 \
    top-two-species-b8-yolov3 \
    > "eval-top-two-species-b8-yolov3.out" \
    2> "eval-top-two-species-b8-yolov3.err"

# CUDA_VISIBLE_DEVICES can be used when running this command as well (see yolo_train_frame_dataset.py documentation for more information)
```

### `gen_detections.py`
This script generates object detection data against a video using a trained model. This detection data is structured as a tuple of two parallel arrays and output in python's native [`pickle`](https://docs.python.org/3/library/pickle.html) format. The output structure is as follows:

```
(
  [ frame_number, ... repeat for each sampled frame in the video ],
  [
    [
        [ frame_number, label, confidence, x1, y1, x2, y2 ],
        [ frame_number, label, confidence, x1, y1, x2, y2 ],
        ... repeat for each detection in the frame 
    ]
    ... repeat for each sampled frame in the video
  ]
)
```

Sampling every frame in the video can be quite slow; a `stride` can be specified which allows you to sample only some of the frames.

Example usages:
```bash
python gen_detections.py \
  --model-path ./training/top-two-species-b8-yolov3/models/detection_model-ex-064--loss-0031.186.h5 \
  --config-path ./training/top-two-species-b8-yolov3/json/detection_config.json \
  --stride 2 \
  --output-path ./detections.pickle \
  ~/fish/videos/A000023_L.avi
```